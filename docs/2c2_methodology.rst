Methodology
###########

KLL will be taking a three-pronged approach in arriving at the requirements of the system. They are briefly decsribed as follows:

1. User needs assessment
^^^^^^^^^^^^^^^^^^^^^^^^

Through a series of contextual interviews, i.e interviews with real people working on the real problem, as well as focussed group discussions the primary purpose of this exercise is for us to:

1. **Test and validate our assumptions about who our end users actually are**.

  - Who are these users and where do they work (CSOs, INGOs, local and central governments)?
  - How similar/different are their data needs (using/analysing data for planning vs. facilitating data collection)?
  - What is the nature of the data that they're looking for? What are some commonly pursued themes (socio economic data, sanitation data, etc)?

2. **Test and  validate our assumptions about how information flows within the users' working environments**.

  - Where does the demand for this data come from?
  - Where did they look for data in the past?
  - How do they look for this data? For instance, will mayors and elected representatives that will be doing the 'looking'? Or do they assign someone else (IT/Information officers) to carry out data related tasks for them?
  - Why are they looking for data in the first place?

    - is it for planning of regional projects such as WASH and literacy programs.
    - is it because of demand from partner organizations working in the area (Water Aid, GIZ, etc.)
    - is it based on internal local governance needs, eg. social security disbursal, identification of earthquake victims that the survey left out?
    - is it all of the above?


3. **Identify and understand different user groups and their relationships with each other**.

  - What is the degree with which end users' abilities (expert vs. novice) vary across identified groups?
  - Is it possible to identify which level (expert vs. novice) we could prioritize and focus on when redesigning the tool?


4. **Test our assumptions about end users' abilities and challenges in using tools such as ODP**.

  - Are end users actually capable of looking for, and arriving at the data they need?
  - Are end-users capable of correctly interpreting and/or consuming the data they arrive at?
  - Do they trust the information? If not why?

5. **Identify end users' training and capacity building needs, and design programs to address the same**.



2. Heuristic evaluation of the existing user interface
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The evaluation exercise is intended to identify and assess problems with the existing user interface. It is hoped that learnings from the user sense-making exercise will pave way for a thorough heuristic evaluation of the currently existing portal on the following three dimensions:

1. **Learnability**: This refers to an assessment of how easy or difficult it is for identified user groups to learn how to use the system. We will be looking at things like:

  - How does the platform expect users to learn about using the system (tinkering vs. how-tos).
  - How does the UI capture user inputs? Is it through command line interactions, menu based interactions, direct feedback and/or search based interactions? Is it a combination of some or all of the above?
  - Given our knowledge of users technical abilities, are the existing mechanisms for user inputs apt? What changes should we make?
  - How similar/consistent are interaction patterns across different sections of the application?
  - What mechanism does the platform provide the users for seeking help? How reliable/accurate is it?


2. **Efficiency**: This refers to an assessment of how quickly can a user arrive at what he wants from the system:

  - Is it easy to move to different pages, different areas of pages and different state of the application?
  - How much difference in time does it take for an experienced user to arrive at what is required as opposed to an inexperienced user?
  - Are there automated actions running behind to reduce the users cognitive load, letting the user to work just on high level tasks? Are they required for this platform?

3. **Safety**: This refers to an assessment of error prone areas of the interface. Things like, but not limited to:

  - Do errors have a consistent behavior?
  - Does the user always know when anything unexpected or unusual happens?
  - Are there appropriate messages or dialog box?
  - Are suggestions and errors in plain language?


Note: The evaluation process can run in parallel to the user sensemaking exercise. During contextual interviews with different potential end-users, we have also tried to encourage the end-user to get a feel for the system by actually using it. These observations have been thoroughly captured in detail the field notes.

3. Consolidation and translation into system needs.
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

After the completion of the user sense-making and the interface evaluation phase, KLL will be working to consolidate all the feedback and inputs received from multiple sources, which comprise of, and is not be limited to:

1. Contextual interviews and focused group discussions in the user sense-making stage,
2. Results from the heuristic evaluation exercise.
3. Feedback received when demonstrating the tool to different end users.
4. Messages received directly through the app through FB Messenger.

After this, taking into account our improved understanding of end-users, their informational needs, as well as technical abilities, we will work on identifying:

1. Usability related improvements that will need to be incorporated to the present system,
2. Functionality related additions/improvements that need to be incorporated,

Results from this exercise will then be passed along to the portal re-design and development phase.
